# 7. Data Services Manual Deployment

## Overview
Stateful data services including PostgreSQL, Redis, and Apache Kafka for application data persistence and messaging.

## Prerequisites
- Foundation workflow completed successfully
- Ingress workflow completed successfully
- kubectl configured for the EKS cluster
- Helm 3.x installed
- Optional: LGTM Observability Stack for monitoring

## Components
- **PostgreSQL**: Relational database with high availability
- **Redis**: In-memory data structure store and cache
- **Apache Kafka**: Distributed streaming platform
- **Schema Registry**: Schema management for Kafka (optional)

## Manual Deployment Steps

### 1. Create Data Services Namespace
```bash
kubectl create namespace data-services
kubectl label namespace data-services app.kubernetes.io/managed-by=manual
```

### 2. Deploy PostgreSQL
```bash
# Add Bitnami Helm repository
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# Deploy PostgreSQL with HA
helm install postgresql bitnami/postgresql \
  --namespace data-services \
  --version 13.2.24 \
  --set auth.postgresPassword=postgres123 \
  --set auth.database=appdb \
  --set auth.username=appuser \
  --set auth.password=appuser123 \
  --set architecture=replication \
  --set primary.persistence.enabled=true \
  --set primary.persistence.size=50Gi \
  --set primary.persistence.storageClass=gp3 \
  --set primary.resources.requests.memory=512Mi \
  --set primary.resources.requests.cpu=250m \
  --set primary.resources.limits.memory=2Gi \
  --set primary.resources.limits.cpu=1000m \
  --set readReplicas.replicaCount=1 \
  --set readReplicas.persistence.enabled=true \
  --set readReplicas.persistence.size=50Gi \
  --set readReplicas.persistence.storageClass=gp3 \
  --set readReplicas.resources.requests.memory=512Mi \
  --set readReplicas.resources.requests.cpu=250m \
  --set readReplicas.resources.limits.memory=2Gi \
  --set readReplicas.resources.limits.cpu=1000m \
  --set metrics.enabled=true \
  --set metrics.serviceMonitor.enabled=true
```

### 3. Deploy Redis
```bash
# Deploy Redis with Sentinel for HA
helm install redis bitnami/redis \
  --namespace data-services \
  --version 18.4.0 \
  --set auth.enabled=true \
  --set auth.password=redis123 \
  --set architecture=replication \
  --set master.persistence.enabled=true \
  --set master.persistence.size=20Gi \
  --set master.persistence.storageClass=gp3 \
  --set master.resources.requests.memory=256Mi \
  --set master.resources.requests.cpu=250m \
  --set master.resources.limits.memory=1Gi \
  --set master.resources.limits.cpu=500m \
  --set replica.replicaCount=2 \
  --set replica.persistence.enabled=true \
  --set replica.persistence.size=20Gi \
  --set replica.persistence.storageClass=gp3 \
  --set replica.resources.requests.memory=256Mi \
  --set replica.resources.requests.cpu=250m \
  --set replica.resources.limits.memory=1Gi \
  --set replica.resources.limits.cpu=500m \
  --set sentinel.enabled=true \
  --set sentinel.resources.requests.memory=64Mi \
  --set sentinel.resources.requests.cpu=100m \
  --set sentinel.resources.limits.memory=128Mi \
  --set sentinel.resources.limits.cpu=200m \
  --set metrics.enabled=true \
  --set metrics.serviceMonitor.enabled=true
```

### 4. Deploy Apache Kafka
```bash
# Deploy Kafka with Zookeeper
helm install kafka bitnami/kafka \
  --namespace data-services \
  --version 26.4.2 \
  --set listeners.client.protocol=PLAINTEXT \
  --set listeners.controller.protocol=PLAINTEXT \
  --set listeners.interbroker.protocol=PLAINTEXT \
  --set listeners.external.protocol=PLAINTEXT \
  --set controller.replicaCount=3 \
  --set controller.persistence.enabled=true \
  --set controller.persistence.size=50Gi \
  --set controller.persistence.storageClass=gp3 \
  --set controller.resources.requests.memory=1Gi \
  --set controller.resources.requests.cpu=250m \
  --set controller.resources.limits.memory=2Gi \
  --set controller.resources.limits.cpu=1000m \
  --set broker.replicaCount=3 \
  --set broker.persistence.enabled=true \
  --set broker.persistence.size=100Gi \
  --set broker.persistence.storageClass=gp3 \
  --set broker.resources.requests.memory=1Gi \
  --set broker.resources.requests.cpu=250m \
  --set broker.resources.limits.memory=2Gi \
  --set broker.resources.limits.cpu=1000m \
  --set zookeeper.enabled=true \
  --set zookeeper.replicaCount=3 \
  --set zookeeper.persistence.enabled=true \
  --set zookeeper.persistence.size=20Gi \
  --set zookeeper.persistence.storageClass=gp3 \
  --set zookeeper.resources.requests.memory=512Mi \
  --set zookeeper.resources.requests.cpu=250m \
  --set zookeeper.resources.limits.memory=1Gi \
  --set zookeeper.resources.limits.cpu=500m \
  --set metrics.kafka.enabled=true \
  --set metrics.jmx.enabled=true \
  --set metrics.serviceMonitor.enabled=true
```

### 5. Deploy Schema Registry (Optional)
```bash
# Add Confluent Helm repository
helm repo add confluentinc https://packages.confluent.io/helm
helm repo update

# Deploy Schema Registry
helm install schema-registry confluentinc/cp-schema-registry \
  --namespace data-services \
  --version 0.6.1 \
  --set kafka.bootstrapServers="kafka:9092" \
  --set replicaCount=2 \
  --set resources.requests.memory=512Mi \
  --set resources.requests.cpu=250m \
  --set resources.limits.memory=1Gi \
  --set resources.limits.cpu=500m
```

## Verification Steps

### 1. Check All Data Service Pods
```bash
kubectl get pods -n data-services
```

### 2. Check Services
```bash
kubectl get svc -n data-services
```

### 3. Check Persistent Volume Claims
```bash
kubectl get pvc -n data-services
```

### 4. Verify PostgreSQL
```bash
# Get PostgreSQL password
kubectl get secret postgresql -n data-services -o jsonpath="{.data.postgres-password}" | base64 --decode

# Port-forward to PostgreSQL
kubectl port-forward svc/postgresql 5432:5432 -n data-services

# Connect using psql (from another terminal)
PGPASSWORD=postgres123 psql -h localhost -U postgres -d appdb
```

### 5. Verify Redis
```bash
# Get Redis password
kubectl get secret redis -n data-services -o jsonpath="{.data.redis-password}" | base64 --decode

# Port-forward to Redis
kubectl port-forward svc/redis-master 6379:6379 -n data-services

# Connect using redis-cli (from another terminal)
redis-cli -h localhost -p 6379 -a redis123
```

### 6. Verify Kafka
```bash
# Port-forward to Kafka
kubectl port-forward svc/kafka 9092:9092 -n data-services

# Create a test topic (from another terminal)
kubectl exec -it kafka-0 -n data-services -- kafka-topics.sh \
  --create \
  --topic test-topic \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 3

# List topics
kubectl exec -it kafka-0 -n data-services -- kafka-topics.sh \
  --list \
  --bootstrap-server localhost:9092
```

## Configuration

### PostgreSQL Configuration
```sql
-- Create application database and user
CREATE DATABASE myapp;
CREATE USER myappuser WITH PASSWORD 'myapppass';
GRANT ALL PRIVILEGES ON DATABASE myapp TO myappuser;

-- Enable required extensions
\c myapp
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
```

### Redis Configuration
```bash
# Basic Redis operations
redis-cli -h redis-master.data-services.svc.cluster.local -p 6379 -a redis123

# Set and get values
SET mykey "Hello World"
GET mykey

# Check Redis info
INFO replication
```

### Kafka Configuration
```bash
# Create application topics
kubectl exec -it kafka-0 -n data-services -- kafka-topics.sh \
  --create \
  --topic user-events \
  --bootstrap-server localhost:9092 \
  --partitions 6 \
  --replication-factor 3

kubectl exec -it kafka-0 -n data-services -- kafka-topics.sh \
  --create \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --partitions 6 \
  --replication-factor 3

# Test producer and consumer
kubectl exec -it kafka-0 -n data-services -- kafka-console-producer.sh \
  --topic user-events \
  --bootstrap-server localhost:9092

kubectl exec -it kafka-0 -n data-services -- kafka-console-consumer.sh \
  --topic user-events \
  --bootstrap-server localhost:9092 \
  --from-beginning
```

## Application Integration

### PostgreSQL Connection
```yaml
# Example application deployment with PostgreSQL
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: myapp
        image: myapp:latest
        env:
        - name: DB_HOST
          value: "postgresql.data-services.svc.cluster.local"
        - name: DB_PORT
          value: "5432"
        - name: DB_NAME
          value: "appdb"
        - name: DB_USER
          value: "appuser"
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgresql
              key: password
```

### Redis Connection
```yaml
# Redis configuration in application
env:
- name: REDIS_HOST
  value: "redis-master.data-services.svc.cluster.local"
- name: REDIS_PORT
  value: "6379"
- name: REDIS_PASSWORD
  valueFrom:
    secretKeyRef:
      name: redis
      key: redis-password
```

### Kafka Connection
```yaml
# Kafka configuration in application
env:
- name: KAFKA_BROKERS
  value: "kafka.data-services.svc.cluster.local:9092"
- name: SCHEMA_REGISTRY_URL
  value: "http://schema-registry.data-services.svc.cluster.local:8081"
```

## Backup and Recovery

### PostgreSQL Backup
```bash
# Create backup
kubectl exec postgresql-0 -n data-services -- pg_dump -U postgres appdb > backup.sql

# Restore backup
kubectl cp backup.sql data-services/postgresql-0:/tmp/backup.sql
kubectl exec -it postgresql-0 -n data-services -- psql -U postgres -d appdb -f /tmp/backup.sql
```

### Redis Backup
```bash
# Redis automatically creates RDB snapshots
# Copy snapshot file
kubectl cp data-services/redis-master-0:/data/dump.rdb ./redis-backup.rdb

# Restore by copying back
kubectl cp ./redis-backup.rdb data-services/redis-master-0:/data/dump.rdb
kubectl exec redis-master-0 -n data-services -- redis-cli DEBUG RESTART
```

### Kafka Backup
```bash
# Kafka topics can be backed up by consuming all messages
kubectl exec -it kafka-0 -n data-services -- kafka-console-consumer.sh \
  --topic user-events \
  --bootstrap-server localhost:9092 \
  --from-beginning \
  --timeout-ms 5000 > user-events-backup.txt
```

## Monitoring and Alerting

### Key Metrics to Monitor
- **PostgreSQL**: Connection count, query performance, replication lag
- **Redis**: Memory usage, hit ratio, replication status
- **Kafka**: Broker health, partition lag, disk usage

### Sample Grafana Queries (if Prometheus is available)
```promql
# PostgreSQL connections
pg_stat_database_numbackends{datname="appdb"}

# Redis memory usage
redis_memory_used_bytes

# Kafka lag
kafka_consumer_lag_sum
```

## Security Best Practices

### PostgreSQL Security
- Use strong passwords
- Enable SSL/TLS connections
- Implement network policies
- Regular security updates

### Redis Security
- Enable authentication
- Use SSL/TLS for connections
- Implement proper network isolation
- Monitor for suspicious activity

### Kafka Security
- Configure SASL authentication
- Enable SSL/TLS encryption
- Implement ACLs for topic access
- Monitor consumer groups

## Cost Considerations
- **PostgreSQL**: Storage ~$15-20/month, compute ~$30-40/month
- **Redis**: Storage ~$10-15/month, compute ~$20-30/month
- **Kafka**: Storage ~$25-35/month, compute ~$40-60/month
- **Total estimated cost**: ~$100-160/month

## Troubleshooting

### PostgreSQL Issues
```bash
# Check PostgreSQL logs
kubectl logs -f postgresql-0 -n data-services

# Check replication status
kubectl exec -it postgresql-0 -n data-services -- psql -U postgres -c "SELECT * FROM pg_stat_replication;"

# Check disk usage
kubectl exec -it postgresql-0 -n data-services -- df -h
```

### Redis Issues
```bash
# Check Redis logs
kubectl logs -f redis-master-0 -n data-services

# Check Redis replication
kubectl exec -it redis-master-0 -n data-services -- redis-cli -a redis123 INFO replication

# Check memory usage
kubectl exec -it redis-master-0 -n data-services -- redis-cli -a redis123 INFO memory
```

### Kafka Issues
```bash
# Check Kafka logs
kubectl logs -f kafka-0 -n data-services

# Check cluster status
kubectl exec -it kafka-0 -n data-services -- kafka-broker-api-versions.sh \
  --bootstrap-server localhost:9092

# Check disk usage
kubectl exec -it kafka-0 -n data-services -- df -h /bitnami/kafka
```

### Common Issues
1. **Storage Full**: Monitor disk usage and configure log retention
2. **Connection Issues**: Check service discovery and network policies
3. **Performance Issues**: Monitor resource usage and tune configurations
4. **Replication Lag**: Check network connectivity and resource constraints

## Cleanup
```bash
helm uninstall schema-registry -n data-services
helm uninstall kafka -n data-services
helm uninstall redis -n data-services
helm uninstall postgresql -n data-services
kubectl delete namespace data-services
```

## Notes
- Data services require significant cluster resources (recommend 4+ nodes)
- Consider using AWS managed services (RDS, ElastiCache, MSK) for production
- Implement proper backup strategies before production use
- Monitor resource usage and adjust configurations based on application needs