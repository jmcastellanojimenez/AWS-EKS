name: "Infrastructure Health Monitoring"
description: "Automated cluster health checks, resource usage analysis, and optimization suggestions"
version: "1.0.0"

trigger:
  type: "hybrid"
  schedule: "0 */4 * * *"  # Every 4 hours (optimized frequency)
  manual_trigger: true
  
  intelligent_triggers:
    - type: "threshold_based"
      condition: "node_cpu > 80% OR node_memory > 85%"
      frequency: "every_5_minutes"
      priority: "high"
      
    - type: "trend_based"
      condition: "resource_usage_increasing_trend > 7_days"
      frequency: "daily"
      priority: "medium"
      
    - type: "anomaly_based"
      condition: "resource_usage_anomaly_detected"
      frequency: "real_time"
      priority: "critical"
  
  events:
    - "node_resource_threshold_exceeded"
    - "pod_restart_frequency_high"
    - "cluster_capacity_warning"
    - "cost_anomaly_detected"
    - "performance_degradation_detected"

configuration:
  optimized_thresholds:
    node_cpu_utilization: 75  # Reduced for earlier detection
    node_memory_utilization: 80  # Reduced for earlier detection
    pod_restart_threshold: 3  # More sensitive to restart loops
    disk_usage_warning: 75  # Earlier warning for storage
    network_latency_threshold: 50  # More sensitive to network issues
    
  adaptive_thresholds:
    enabled: true
    learning_period: "7_days"
    adjustment_factor: 0.1
    
  performance_optimization:
    parallel_execution: true
    max_concurrent_checks: 5
    timeout_per_check: "30s"
    result_caching: "5m"
  
  monitoring_scope:
    namespaces:
      - "kube-system"
      - "observability" 
      - "ambassador"
      - "cert-manager"
      - "external-dns"
      - "ecotrack"
    
    resource_types:
      - "nodes"
      - "pods"
      - "deployments"
      - "services"
      - "persistentvolumes"
      - "persistentvolumeclaims"

actions:
  - name: "cluster_health_check"
    description: "Comprehensive cluster health assessment"
    commands:
      - "kubectl get nodes -o wide"
      - "kubectl top nodes"
      - "kubectl get pods -A --field-selector=status.phase!=Running"
      - "kubectl get events -A --sort-by='.lastTimestamp' | tail -20"
    
  - name: "resource_usage_analysis"
    description: "Analyze resource usage patterns and identify optimization opportunities"
    commands:
      - "kubectl top pods -A --sort-by=memory"
      - "kubectl top pods -A --sort-by=cpu"
      - "kubectl describe nodes | grep -E '(Allocated resources|Resource.*Requests.*Limits)'"
    
  - name: "capacity_monitoring"
    description: "Monitor node capacity and scaling recommendations"
    commands:
      - "kubectl get nodes -o json | jq '.items[] | {name: .metadata.name, capacity: .status.capacity, allocatable: .status.allocatable}'"
      - "kubectl get hpa -A"
      - "kubectl describe hpa -A"
    
  - name: "persistent_volume_check"
    description: "Check persistent volume status and usage"
    commands:
      - "kubectl get pv"
      - "kubectl get pvc -A"
      - "df -h /var/lib/kubelet/pods"
    
  - name: "network_health_check"
    description: "Verify network connectivity and DNS resolution"
    commands:
      - "kubectl get svc -A"
      - "kubectl get ingress -A"
      - "kubectl exec -n kube-system deployment/coredns -- nslookup kubernetes.default.svc.cluster.local"

optimization_suggestions:
  - name: "node_scaling_recommendations"
    condition: "node_cpu_utilization > 80 OR node_memory_utilization > 85"
    suggestion: "Consider scaling up the cluster or optimizing resource requests/limits"
    
  - name: "pod_resource_optimization"
    condition: "pod_cpu_usage < 30% of requests for 24h"
    suggestion: "Reduce CPU requests for underutilized pods to improve cluster efficiency"
    
  - name: "storage_optimization"
    condition: "pv_usage < 50% for 7 days"
    suggestion: "Consider reducing persistent volume sizes or implementing storage lifecycle policies"
    
  - name: "replica_scaling_optimization"
    condition: "hpa_current_replicas == hpa_min_replicas for 48h"
    suggestion: "Consider reducing minimum replicas for cost optimization"

alerts:
  - name: "high_node_utilization"
    condition: "node_cpu_utilization > 90 OR node_memory_utilization > 95"
    severity: "critical"
    message: "Node resource utilization is critically high"
    
  - name: "pod_restart_loop"
    condition: "pod_restart_count > 10 in 1h"
    severity: "warning"
    message: "Pod is experiencing frequent restarts"
    
  - name: "persistent_volume_full"
    condition: "pv_usage > 90%"
    severity: "warning"
    message: "Persistent volume is approaching capacity limit"
    
  - name: "cluster_capacity_exceeded"
    condition: "pending_pods > 0 for 10m"
    severity: "critical"
    message: "Cluster lacks capacity to schedule pending pods"

reporting:
  format: "markdown"
  include_metrics: true
  include_recommendations: true
  retention_days: 30
  
  sections:
    - "cluster_overview"
    - "node_health"
    - "pod_status"
    - "resource_utilization"
    - "capacity_analysis"
    - "optimization_recommendations"
    - "cost_impact_analysis"

integration:
  prometheus:
    enabled: true
    metrics_endpoint: "http://prometheus-server.observability.svc.cluster.local:80"
    custom_queries:
      - "node_cpu_utilization: (1 - avg(rate(node_cpu_seconds_total{mode='idle'}[5m]))) * 100"
      - "node_memory_utilization: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100"
      - "pod_restart_rate: increase(kube_pod_container_status_restarts_total[1h])"
  
  grafana:
    enabled: true
    dashboard_url: "http://grafana.observability.svc.cluster.local:3000"
    create_annotations: true
  
  slack:
    enabled: false
    webhook_url: "${SLACK_WEBHOOK_URL}"
    channel: "#infrastructure-alerts"

cost_analysis:
  enabled: true
  spot_instance_monitoring: true
  resource_waste_calculation: true
  optimization_savings_estimation: true
  
  metrics:
    - "cost_per_cpu_hour"
    - "cost_per_memory_gb_hour" 
    - "unused_resource_cost"
    - "spot_instance_savings"
    - "storage_lifecycle_savings"